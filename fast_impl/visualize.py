# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/visualize.grad_cam.ipynb (unless otherwise specified).

__all__ = ['show_heatmap', 'create_batch', 'GuidedBackprop']

# Cell
from fastai2.vision.all import *
from .core import arch_summary, get_module, min_max_scale
import gc

# Cell
def show_heatmap(cam,sz,ax=None,alpha=0.6,interpolation='bilinear',
                 cmap='magma'):
  show_image(min_max_scale(cam),ax=ax,
             extent=(1,sz,sz,1),alpha=alpha,
             interpolation=interpolation,cmap=cmap)

# Cell
def create_batch(dls:DataLoaders,fname,lbl_idx,size=None,method=None):
  """Create test_batch from filename and label index
  Refer `dls.vocab` to find validation index
  In case you want to Resize an image, use `size` and `method` parameters
  Default method of cropping is set to 'squish' cosidering the use-case
  """
  img = PILImage.create(fname)
  if size is not None:
    method = ifnone(method,'squish')
    resize = Resize(size,method=method)
    img = resize(img)
  xb, = first(dls.test_dl([img]))
  yb = dls.categorize(dls.vocab[lbl_idx])[None]
  return xb,yb

# Cell
class GuidedBackprop:
    "Produces gradients generated with guided back propagation from the given image"
    def __init__(self, model,act_cls:nn.Module=nn.ReLU):
        self.model = model.requires_grad_().eval()
        self.act_cls = act_cls
        self._fmaps = L()
        self._hooks = L()
        self._register_hooks()

    def hook(self,module,hook_fn,grad=False):
      if not grad:
        self._hooks += module.register_forward_hook(hook_fn)
      else:
        self._hooks += module.register_backward_hook(hook_fn)

    def _register_hooks(self):
        """Updates relu activation functions so that
        1- stores output in forward pass
        2- imputes zero for gradient values that are less than zero
        """
        def store_activations_hook(module,input,output):
            "Store forward pass outputs (Activaton Maps)"
            self._fmaps += output


        def clamp_gradients_hook(module,grad_in, grad_out):
            "If there is a negative gradient, change it to zero"
            # Get last forward output
            fmap = self._fmaps[-1]
            fmap[fmap > 0] = 1
            grad_out_new = fmap * F.relu(grad_in[0])
            del self._fmaps[-1]  # Remove last forward output
            return (grad_out_new,)

        # hook up ReLUs (activation layers)
        for name, module in self.model.named_modules():
            if isinstance(module, self.act_cls):
              self.hook(module,store_activations_hook)
              self.hook(module,clamp_gradients_hook,grad=True)

    def remove_hooks(self):
      for hook in self._hooks:
        hook.remove()
      gc.collect()

    def guided_backprop(self,xb,yb=None):
      xb = xb.clone() # To avoid modifying original tensors
      first_conv = flatten_model(self.model)[0]

      def hook_first(m,grad_in,grad_out): return grad_in[0]

      with Hook(first_conv,hook_first,is_forward=False) as reconstruction:
        xb.requires_grad_()
        if not xb.grad is None:
          xb.grad.zero_()
        y_preds = self.model(xb)
        self.model.zero_grad()
        if yb is None:
          y = y_preds.argmax().item()
        else: y = yb.item()
        y_preds[0,y].backward(retain_graph=True)
        gbprop = reconstruction.stored[0]
        self.remove_hooks()
        return gbprop